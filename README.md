# ANNOTATION-TRAIN
# ğŸ‘‹ Hi, I'm Samuel!  

I'm a highly skilled Data Annotator with over three years of experience in text, image, audio, and video annotation. I specialise in preparing high-quality training data for AI models, ensuring precision and consistency in data labelling.  

 ğŸ”¹ My Expertise:  
âœ” AI training data preparation (text, image, audio, video)  
âœ” Annotation tools & methodologies   
âœ” Certified by **Softechub**  

## ğŸ“Œ VIDEO ANNOTATION PROJECTS  
Video Annotation â€“ Football Activity Detection, Emotion & Interaction Labelling, Extreme stunt.

ğŸ”¹Project Type
Freelance Practical Task / Annotation Assignment (via Softechub)

ğŸ”¹Project Overview
This project involved frame-by-frame video annotation to train computer vision and machine learning models in recognising specific actions, emotions, or interactions. The goal was to create accurate and consistent labels across key moments to support AI models in learning visual patterns over time.

**In the Football Annotation Project**
actions such as passing, tackling, dribbling and fouls were carefully tagged based on timecodes, it contains a frame count of 270.

**In the "Lovers" Annotation Assignment,** 
we focused on identifying  body language and interpersonal interactions like hugging and holding hands, and it contains 301 frame counts.

**Extreme Stunt Cyclist Annotation (Practical Task):**
Involved in annotating high-speed stunts and risky manoeuvres performed by cyclists, which shows Key events like take-offs, mid-air flips, landings, and crash points. This helped in building models for action recognition. it contains 442 frame counts.

**Tasks Performed**
Watched videos carefully and segmented them into scenes or moments of interest.
Annotated key actions, behaviours, and emotional cues using bounding boxes.
Labelled human interactions and contextual behaviours frame by frame.
Ensured annotation consistency across scenes.

**Platform Used**
CVAT
**Outcomes / Results**
Supported the development of datasets used for sports analysis, emotion detection, or human behaviour modelling.
Received feedback from reviewers on LinkedIn/trainers at Softechub. 
https://www.linkedin.com/posts/samuel-olawuyi-6a476418a_dataannotation-aitrainingdata-machinelearning-activity-7302769147925389312-_FqR?utm_source=share&utm_medium=member_android&rcm=ACoAACybONwBMH47i_c15FzkZcCAcAyQ1zfgL1A.
https://www.linkedin.com/posts/samuel-olawuyi-6a476418a_ai-machinelearning-artificialintelligence-activity-7305571893032419328-WME-?utm_source=share&utm_medium=member_android&rcm=ACoAACybONwBMH47i_c15FzkZcCAcAyQ1zfgL1A.

Skills Demonstrated
Video annotationÂ· Sports action tagging Â· Frame-by-frame attention Â· Label consistency Â· 

   

## ğŸ“ TEXT ANNOTATION PROJECTS
Client: Remote Freelance Project
Role: Text Annotator
Tools/Platforms: Centific Proprietary Annotation platform
Date: 2022-2025 

ğŸ“ **Project Type:** sentiment analysis  
ğŸ”¹ Project Overview: This project involved sentiment analysis on AI-generated responses. Due to confidentiality agreements, specific instructions cannot be shared. However, the core task focused on categorising prompts and corresponding outputs into positive, negative, or neutral sentiments. The goal was to help improve the modelâ€™s ability to accurately detect emotionally relevant and contextually appropriate responses, ultimately enhancing the modelâ€™s understanding of user intent and emotional tone.

**ğŸ”¹TaskPerformed**
Annotated prompts and AI responses with sentiment tags (positive, negative, neutral).
Used proprietary annotation tools with strict accuracy guidelines.
Reviewed complex or borderline samples to ensure correct sentiment categorisation.
Reported edge cases to annotation leads for clarification and resolution.
Maintained 100% task submission rates throughout the project.

**ğŸ”¹Results**.
Labelled over 2,000 text samples with a QA accuracy score above 90%.
Acknowledged for consistency and quality during internal audits.
Contributed to smoother workflow by sharing suggestions on ambiguous labels.

**Confidentiality Note**
Due to NDA terms, specific dataset details and instruction content have been omitted.


## ğŸ“ IMAGE ANNOTATION PROJECTS.
Client: Freelance Evaluation Task- Image Annotation (Remote)
Role: Image Annotator
Tools/Platforms: Cvat
Date: April 2025
ğŸ“ **Project Type:**  This task involved pixel-level annotation of objects within images using a brush-based segmentation tool. Each visible object was required to be individually segmented and labelled using a predefined list of classes. Annotations were made with minimal overlap by utilising the â€œRemove Underlying Pixelsâ€ function to ensure mask clarity and accuracy. The ShowBitmap feature was used to maintain unannotated background pixels for better visual separation.
Objects located behind glass surfaces were excluded from annotation to preserve data quality and follow task guidelines. In cases of unavoidable overlap, layer-based annotation was applied to differentiate between foreground and background elements.

**ğŸ”¹Tasks Performed**
Manually segmented and labelled objects within images based on a given class list.
Ensured clean annotations with minimal mask overlap.
Skipped annotation of objects behind transparent surfaces (e.g., glass).
Applied advanced techniques like layer-based segmentation when required.
Followed task instructions strictly to maintain consistency and quality.

**ğŸ”¹Results**
Completed and submitted the task within the evaluation timeframe.
Gained practical experience in brush-based segmentation techniques and visual QA standards.

**Confidentiality Note**
There weren't any form of NDA terms, which is why I have the confidence to share the link to
The project I worked on
https://drive.google.com/drive/folders/144iwWucJQx5nkxJ7TvkBpRjDIuWG8FrM?usp=sharing.

**Skills Demonstrated**
Image annotation Â· Object segmentation Â· Attention to detail Â· Class labelling Â· Visual consistency Â· Layer management Â· Data labelling

**Challenges**
Challenge: Distinguishing similar objects in cluttered scenes.
Solution: Zoomed in and used layer-based annotations to keep masks separate.

Challenge: Mask bleeding and pixel overlap.
Solution: Used the â€œRemove Underlying Pixelsâ€ function efficiently to maintain clean separations.


## ğŸµ Audio Annotation Projects  
 Project Title
Interview Audio Annotation â€“ Speaker Differentiation & Turn-Taking Detection
Project Type
Practical Assignment (via Softechub)
**Project Overview**
This project involved annotating a recorded interview between two speakers using Annotation Pro, with the goal of improving AI's ability to understand speaker roles, conversation flow, and turn-taking dynamics. The task was structured to support machine learning models in recognising when and how different individuals participate in a dialogue.

**Tasks Performed**
Identified and labelled speakers
Annotated clear speaker turns, marking who was speaking and when the other was silent.
Ensured high precision in detecting pauses, interruptions, and speaker transitions.
Reviewed audio carefully to avoid overlap errors or misattributed speech.

**Tools & Platforms Used**
Annotation Pro â€“ for precise time-based speaker tagging and segmentation.

**Outcomes / Results**
Successfully differentiated between two speakers with accurate role assignment.
Delivered clean, structured annotations that support training of dialogue systems, speaker diarization models, and AI transcription tools.
Demonstrated strong attention to detail and ability to handle real-world, unscripted conversations.

**Confidentiality Note**
As this was a structured assignment for educational purposes, no private or client-specific data is included. The project reflects simulated real-world annotation conditions.

**Skills Demonstrated**
Audio annotation Â· Speaker diarization Â· Turn-taking detection Â· Role labelling Â· Timestamped segmentation Â· Use of Annotation Pro Â· Detail-oriented analysis

Result.
![image](https://github.com/user-attachments/assets/21b87460-bbbd-410b-ab1d-393d84237d5e)





